{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Process Train Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = pd.read_csv(\"../Dataset_DAY1/Data/train_set.csv\", delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## drop features\n",
    "def Drop_unneed_columns(dataset):\n",
    "    cols= ['application_ID', 'company_ID', 'decision_date']\n",
    "    dataset= dataset.drop(columns=cols)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Nan_values(dataset):\n",
    "    column_names = dataset.columns.tolist()\n",
    "    drop_columns = []\n",
    "    for name in column_names:\n",
    "        nan_count = dataset[name].isna().sum()\n",
    "        print(f\"column {name}: {nan_count}\")\n",
    "        if (nan_count/28000) > 0.5:\n",
    "            print(f\"Number of NaN values in column '{name}': {nan_count}\")\n",
    "            drop_columns.append(name)\n",
    "    return drop_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Replace_cate_to_value(column_name, dataset):\n",
    "    # Extract categories\n",
    "\n",
    "    # Extract unique category names from the column\n",
    "    unique_categories = dataset[column_name].unique()\n",
    "\n",
    "    # convert 'numpy.ndarray' in to a python list\n",
    "    l = unique_categories.tolist()\n",
    "    \n",
    "    if 'MISSING' in l:\n",
    "        l.remove('MISSING')\n",
    "        l.sort(reverse=True)\n",
    "\n",
    "    print(l)\n",
    "    \n",
    "    dic = { l[i]:i+1 for i in range(0, len(l))}\n",
    "\n",
    "    # Replace values in the column based on the dictionary mapping\n",
    "    dataset[column_name] = dataset[column_name].replace(dic)\n",
    "    return dic, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Category_values(dataset):\n",
    "    column_names = ['industry_sector','region', 'geo_area','external_score_ver03', 'province','juridical_form']\n",
    "    dic = {}\n",
    "    for column_name in column_names:\n",
    "        category_dic, dataset = Replace_cate_to_value(column_name, dataset)\n",
    "        dic[column_name] = category_dic\n",
    "    return dic, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Replace_bool_toNumbers(dataset):\n",
    "    dataset['cr_available'] = [int(dataset['cr_available'][i]) for i in range(len(dataset['cr_available']))]\n",
    "    dataset['cr_available']\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_var03(dataset):\n",
    "    s0, s1, c0, c1 = 0,0,0,0\n",
    "    # unique_labels = dataset['target'].unique()\n",
    "    for index, row in dataset.iterrows():\n",
    "        if row['external_score_ver03'] != 'MISSING':\n",
    "            if row['target'] == 0:\n",
    "                s0 += row['external_score_ver03']\n",
    "                c0 +=1\n",
    "            elif row['target'] == 1:\n",
    "                s1 +=  row['external_score_ver03']\n",
    "                c1 += 1\n",
    "\n",
    "    m0 = round(s0/c0)\n",
    "    m1 = round(s1/c1)\n",
    "    print(m0)\n",
    "    print(m1)\n",
    "    return m0,m1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Replace_missing(dataset, m0, m1):\n",
    "    # Assuming df is your DataFrame and 'column_to_change' is the column you want to change\n",
    "    # 'condition_column' is the column based on which you want to change the content\n",
    "    dataset.loc[(dataset['target'] == 1) & (dataset['external_score_ver03'] == 'MISSING'), 'external_score_ver03'] = m1\n",
    "    dataset.loc[(dataset['target'] == 0) & (dataset['external_score_ver03'] == 'MISSING'), 'external_score_ver03'] = m0\n",
    "    dataset['external_score_ver03']\n",
    "\n",
    "    # For example, if you want to change the content of 'column_to_change' to 'new_value' where 'condition_column' is True\n",
    "    # Replace 'new_value', 'column_to_change', and 'condition_column' with your actual values\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns \n",
    "train_dataset = Drop_unneed_columns(train_dataset)\n",
    "drop_columns = Nan_values(train_dataset)\n",
    "train_dataset = train_dataset.drop(columns=drop_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace bool values to numerical ones \n",
    "category_dics, train_dataset = Category_values(train_dataset)\n",
    "train_dataset = Replace_bool_toNumbers(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v03 column with missing values \n",
    "m0, m1= mean_var03(train_dataset)\n",
    "train_dataset = Replace_missing(train_dataset, m0, m1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_data(dataset):\n",
    "    # Replace commas with periods in all columns\n",
    "    dataset = dataset.replace(',', '.', regex=True)\n",
    "    print(dataset.dtypes)\n",
    "    dataset = dataset.astype('float32')\n",
    "\n",
    "    # check if the dataset has any nan value\n",
    "    has_nan_values = dataset.isna().any().any()\n",
    "\n",
    "    if has_nan_values:\n",
    "        print(\"DataFrame contains NaN values.\")\n",
    "    else:\n",
    "        print(\"DataFrame does not contain any NaN values.\")\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = normalized_data(train_dataset)\n",
    "train_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train_dataset['days_to_default'].to_numpy() # labels\n",
    "X = train_dataset.drop(columns=['days_to_default']).to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"yellow\"> SVM Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Assuming Y contains the target variable for regression\n",
    "\n",
    "# Standardize the features (mean=0 and variance=1)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(train_dataset.drop(columns='days_to_default'))\n",
    "\n",
    "\n",
    "# Standardize the target variable Y\n",
    "Y = train_dataset['days_to_default']\n",
    "scaler_Y = StandardScaler()\n",
    "Y_scaled = scaler_Y.fit_transform(Y.values.reshape(-1, 1))  # Reshape Y to be a 2D array for StandardScaler\n",
    "\n",
    "\n",
    "# Create PCA object\n",
    "pca = PCA(n_components=20)  # Specify the number of components (desired dimensionality)\n",
    "\n",
    "# Fit PCA to the standardized data and transform the data\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_pca, Y, test_size=0.1, random_state=2)\n",
    "\n",
    "# Create SVR (Support Vector Regression) model\n",
    "regressor = SVR(C=0.1, kernel='linear', gamma='scale')\n",
    "\n",
    "# Fit the model on the training data\n",
    "regressor.fit(X_train, Y_train)\n",
    "\n",
    "# Predict on the testing data\n",
    "Y_pred = regressor.predict(X_test)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE) as a metric\n",
    "mse = mean_squared_error(Y_test, Y_pred)\n",
    "print(\"Mean Squared Error:\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"yellow\"> Random Forest Training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
